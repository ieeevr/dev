ids;contactauthor;email;paperID;paperurl;title;abstract
TVCG2;Yann Moullec;yann.moullec@inria.fr;TVCG-2023-12-0860;;To use or not to use viewpoint oscillations when walking in VR ? State of the art and perspectives;"Viewpoint oscillations are periodic changes in the position and/or orientation of the point of view in a virtual environment. They can be implemented in Virtual Reality (VR) walking simulations to make them feel closer to real walking. This is especially useful in simulations where users remain in place because of space or hardware constraints. As for today, it remains unclear what exact benefit they bring to user experience during walking simulations, and with what characteristics they should be implemented. To answer these questions, we conduct a systematic literature review focusing on five main dimensions of user experience (walking sensation, vection, cybersickness, presence and embodiment) and discuss 44 articles from the fields of VR, Vision, and Human-Computer Interaction. Overall, the literature suggests that viewpoint oscillations benefit vection, and with less evidence, walking sensation and presence. As for cybersickness, the literature contains contrasted results. Based on these results, we recommend using viewpoint oscillations in applications that require accurate distance or speed perception, or that aim to provide compelling walking simulations without a walking avatar, and a particular attention should be paid to cybersickness. Taken together, this work gives recommendations for enhancing walking simulations in VR, which may be applied to entertainment, virtual visits, and medical rehabilitation."
TVCG3;Xiaodan Hu;hu.xiaodan.ht1@is.naist.jp;TVCG-2023-08-0488.R1;;Perception-driven Soft-Edge Occlusion for Optical See-Through Head-Mounted Displays;"Systems with occlusion capabilities, such as those used in vision augmentation, image processing, and optical see-through head-mounted display (OST-HMD), have gained popularity. Achieving precise (hard-edge) occlusion in these systems is challenging, often requiring complex optical designs and bulky volumes. On the other hand, utilizing a single transparent liquid crystal display (LCD) is a simple approach to create occlusion masks. However, the generated mask will appear defocused (soft-edge) resulting in insufficient blocking or occlusion leakage. In our work, we delve into the perception of soft-edge occlusion by the human visual system and present a preference-based optimal expansion method that minimizes perceived occlusion leakage. In a user study involving 20 participants, we made a noteworthy observation that the human eye perceives a sharper edge blur of the occlusion mask when individuals see through it and gaze at a far distance, in contrast to the camera system's observation. Moreover, our study revealed significant individual differences in the perception of soft-edge masks in human vision when focusing. These differences may lead to varying degrees of demand for mask size among individuals. Our evaluation demonstrates that our method successfully accounts for individual differences and achieves optimal masking effects at arbitrary distances and pupil sizes."
TVCG4;Takato Mizuho;takato@cyber.t.u-tokyo.ac.jp;TVCG-2024-04-0262.R1;;Multiple Self-Avatar Effect: Effects of Using Diverse Self-Avatars on Memory Acquisition and Retention of Sign-Language Gestures;"This study proposes a new learning method that employs multiple embodied self-avatars during learning, to use the potential benefit of virtual reality (VR) for effective learning and training. In this study, by taking advantage of the benefit of virtual reality (VR), we propose a new learning method that employs multiple embodied self-avatars during learning. Based on the multiple-context effect, which posits that learning in diverse situations can prevent forgetting and enhance memory retention, we conducted a between-participants study under two conditions: the varied avatar condition, in which participants learned sign languages with different self-avatars in six iterations, and the constant avatar condition, in which the same self-avatar was used consistently. We employed sign language as a learning material that naturally draws attention to self-avatars and is suitable for investigating the effects of varying self-avatars. Initially, the varied avatar condition performed worse than the constant avatar condition. However, in a test conducted after one week in the real world, the varied avatar condition showed significantly less forgetting and better retention than the constant avatar condition. Furthermore, our results suggested a positive correlation between the degree of embodiment toward the avatars and the effectiveness of the proposed method. This study presents an innovative design approach for the use of self-avatars in VR-based education."
TVCG5;Nana Tian;nana.tian@epfl.ch;;https://ieeexplore.ieee.org/document/10510640;The Least Increasing Aversion (LIA) Protocol: Illustration on Identifying Individual Susceptibility to Cybersickness Triggers;"This paper introduces the Least Increase aversion (LIA) protocol to investigate the relative impact of factors that may trigger cybersickness. The protocol is inspired by the Subjective Matching methodology (SMT) from which it borrows the incremental construction of a richer VR experience, except that the full-blown target experience may cause undesired discomfort. In the first session, the participant briefly encounter all factors at the maximum level. Then in the second session they start with the minimum level of all factors as a Baseline. Subsequently, we expect the participant to minimize their exposure to the most adverse factors. This approach ranks the factors from mildest to worst and helps detect individual susceptibility to cybersickness triggers.To validate the applicability of LIA protocol, we further evaluate it with an experiment to identify individual susceptibility to three rotational axes (Yaw, Pitch, and Roll). The findings not only confirm the protocol's capability to accurately discern individual rankings of various factors to cybersickness but also indicate that individual susceptibility is more intricate and multifaceted than initially anticipated"
TVCG6;Sang-Bin Jeon;ludens0508@yonsei.ac.kr;;https://ieeexplore.ieee.org/abstract/document/10470380;F-RDW: Redirected Walking With Forecasting Future Position;"In order to serve better VR experiences to users, existing predictive methods of Redirected Walking (RDW) exploit future information to reduce the number of reset occurrences. However, such methods often impose a precondition during deployment, either in the virtual environment's layout or the user's walking direction, which constrains its universal applications. To tackle this challenge, we propose a novel mechanism F-RDW that is twofold: (1) forecasts the future information of a user in the virtual space without any assumptions, and (2) fuse this information while maneuvering existing RDW methods. The backbone of the first step is an LSTM-based model that ingests the user's spatial and eye-tracking data to predict the user's future position in the virtual space, and the following step feeds those predicted values into existing RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respecting their internal mechanism in applicable this http URL results of our simulation test and user study demonstrate the significance of future information when using RDW in small physical spaces or complex environments. We prove that the proposed mechanism significantly reduces the number of resets and increases the traveled distance between resets, hence augmenting the redirection performance of all RDW methods explored in this work."
TVCG8;Ryosuke Hori;hori-rysk@keio.jp;;10.1109/TVCG.2024.3462816;EventPointMesh: Human Mesh Recovery Solely From Event Point Clouds;
TVCG9;Juan Pieschacon;juan.pieschacon@mymail.unisa.edu.au;;https://doi.org/10.1109/TVCG.2024.3472837;Smart Pipette: Elevating Laboratory Performance with Tactile Authenticity and Real-Time Feedback;"Mastering the correct use of laboratory equipment is a fundamental skill for undergraduate science students involved in laboratory-based training. However, hands-on laboratory time is often limited, and remote students may struggle as their absence from the physical lab limits their skill development. An air-displacement micropipette was selected for our initial investigation, as accuracy and correct technique are essential in generating reliable assay data. Handling small liquid volumes demands hand dexterity and practice to achieve proficiency. This research assesses the importance of tactile authenticity during training by faithfully replicating the micropipette's key physical and operational characteristics. We developed a custom haptic training approach called 'Smart Pipette' which promotes accurate operation and enhances laboratory dexterity training. A comparative user study with 34 participants evaluated the effectiveness of the Smart Pipette custom haptic device against training with off-the-shelf hardware, specifically the Quest VR hand controller, which was chosen because it is held mid-air similar to a laboratory micropipette. Both training conditions are integrated with the same self-paced virtual simulation displayed on a computer screen, offering clear video instructions and realtime guidance. Results demonstrated that participants trained with the Smart Pipette custom haptic exhibited increased accuracy and precision while making fewer errors than those trained with off-the-shelf hardware. The Smart Pipette and the Quest VR controller had no significant differences in cognitive load and system usability scores. Tactile authentic interaction devices address challenges faced by online learners, while their applicability extends to traditional classrooms, where real-time feedback significantly enhances overall training performance outcomes."
TVCG11;Lendy Mulot;lendy.mulot@irisa.fr;TVCG-2023-09-0598.R2;https://ieeexplore.ieee.org/document/10568397;Bimanual Ultrasound Mid-Air Haptics for Virtual Reality Manipulation;"The ability to manipulate and physically feel virtual objects without any real object being present and without equipping the user has been a long-standing goal in virtual reality (VR). Emerging ultrasound mid-air haptics (UMH) technology could potentially address this challenge, as it enables remote tactile stimulation of unequipped users. However, to date, UMH has received limited attention in the field of haptic exploration and manipulation in virtual environments. Existing work has primarily focused on interactions requiring a single hand and thus the delivery of unimanual haptic feedback. Despite being fundamental to a large part of haptic interactions with our environments, bimanual tasks have rarely been studied in the field of UMH interaction in VR. In this paper, we propose the use of non-coplanar mid-air haptic devices for providing simultaneous tactile feedback to both hands during bimanual VR manipulation. We discuss coupling schemes and haptic rendering algorithms for providing bimanual haptic feedback in bimanual interactions with virtual environments. We then present two human participant studies, assessing the benefits of bimanual ultrasound haptic feedback in a two-handed grasping and holding task and in a shape exploration task. Results suggest that the use of multiple non-coplanar UMH devices could be an interesting approach for enriching unencumbered haptic manipulation in virtual environments.'"
TVCG12;Davide Calandra;davide.calandra@polito.it;TVCG-2024-05-0354;https://ieeexplore.ieee.org/document/10643695;A Testbed for Studying Cybersickness and its Mitigation in Immersive Virtual Reality;"Cybersickness (CS) represents one of the oldest problems affecting Virtual Reality (VR) technology. In an attempt to resolve or at least limit this form of discomfort, an increasing number of mitigation techniques have been proposed by academic and industrial researchers. However, the validation of such techniques is often carried out without grounding on a common methodology, making the comparison between the various works in the state of the art difficult. To address this issue, the present article proposes a novel testbed for studying CS in immersive VR and, in particular, methods to mitigate it. The testbed consists of four virtual scenarios, which have been designed to elicit CS in a targeted and predictable manner. The scenarios, grounded on available literature, support the extraction of objective metrics about user's performance. The testbed additionally integrates an experimental protocol that employs standard questionnaires as well as measurements typically adopted in state-of-the-art practice to assess levels of CS and other subjective aspects regarding User Experience. The article shows a possible use case of the testbed, concerning the evaluation of a CS mitigation technique that is compared with the absence of mitigation as baseline condition."
TVCG13;Tamon Miyake;tamontamonc3@gmail.com;;https://ieeexplore.ieee.org/document/10587136/;Development and Evaluation of a Treadmill-based Video-see-through and Optical-see-through Mixed Reality Systems for Obstacle Negotiation Training;"Mixed reality (MR) technologies have a high potential to enhance obstacle negotiation training beyond the capabilities of existing physical systems. Despite such potential, the feasibility of using MR for obstacle negotiation on typical training treadmill systems and its effects on obstacle negotiation performance remains largely unknown. This research bridges this gap by developing an MR obstacle negotiation training system deployed on a treadmill, and implementing two MR systems with a video see-through (VST) and an optical see-through (OST) Head Mounted Displays (HMDs). We investigated the obstacle negotiation performance with virtual and real obstacles. The main outcomes show that the VST MR system significantly changed the parameters of the leading foot in cases of Box obstacle (approximately 22 cm to 30 cm for stepping over 7cm-box), which we believe was mainly attributed to the latency difference between the HMDs. In the condition of OST MR HMD, users tended to not lift their trailing foot for virtual obstacles (approximately 30 cm to 25 cm for stepping over 7cm-box). Our findings indicate that the low-latency visual contact with the world and the user's body is a critical factor for visuo-motor integration to elicit obstacle negotiation."
TVCG14;Lizhi Zhao;lizhizhao@buaa.edu.cn;;10.1109/TVCG.2024.3516778;GaussianHand: Real-Time 3D Gaussian Rendering for Hand Avatar Animation;"Rendering animatable and realistic hand avatars is pivotal for enhancing user experiences in human-centered AR/VR applications. While recent initiatives have utilized neural radiance fields to forge hand avatars with lifelike appearances, these methods are often hindered by high computational demands and the necessity for extensive training views. In this paper, we introduce GaussianHand, the first Gaussian-based real-time 3D rendering approach that enables efficient free-view and free-pose hand avatar animation from sparse view images. Our approach encompasses two key innovations. We first propose Hand Gaussian Blend Shapes that effectively models hand surface geometry while ensuring consistent appearance across various poses. Secondly, we introduce the Neural Residual Skeleton, equipped with Residual Skinning Weights, designed to rectify inaccuracies involved in Linear Blend Skinning deformations due to geometry offsets. Experiments demonstrate that our method not only achieves far more realistic rendering quality with as few as 5 or 20 training views, compared to the 139 views required by existing methods, but also excels in efficiency, achieving up to 125 frames per second for real-time rendering and remarkably surpassing recent methods."
TVCG15;Yue Liu;liuyue@bit.edu.cn;TVCG-2024-07-0521;;Utilizing Gaze-Contingent Rendering to Maintain Visual Attention in Educational VR;"In educational Virtual Reality (VR) environments, objects irrelevant to learning can lead to students' inattention, which adversely affects learning. However, removing these objects from virtual scenes is not feasible, as they are crucial for creating a realistic and immersive experience. Balancing the need to maintain students' attention while preserving the integrity of scenarios is a challenging task. In this paper, we introduce a gaze-contingent rendering (GCR) technique to address such an issue, which is independent of specific elements or configurations in virtual scenes and adaptable across various contexts. Specifically, we utilize gaze-aware rendering adjustments to adaptively reduce the visibility of objects irrelevant to learning while highlighting relevant ones. We develop three GCR strategies (i.e., blur, pixelation, and underexposure) and investigate how these strategies affect students' visual attention, academic achievement, and perceptions of the learning activity across different scenarios. Our findings indicate that the proposed rendering strategies effectively achieve the goals of sustaining visual attention and improving academic achievement without significantly impacting immersion or engagement. As an initial exploration of GCR for maintaining attention within educational VR, this study may inspire new directions in future research on GCR and visual attention maintenance in immersive VR."
