"""Booth ID""",Authors,Abstract,Title,"""Poster Category""","""Video Link"""
1201,"""Lei He: Beihang University; Mingbo Hu: Beihang University; Hongyu Wu: Beihang University; Shuai Li: Beihang University; Hong Qin: Stony Brook University; Aimin Hao: Beihang University""","""Haptic rendering and soft body deformation are important part for virtual surgery. In hysteroscopic surgery, movement of surgical tool is limited to the fixed entry point and there is no well-developed algorithm for these surgery scenes. We propose an 4 domain of freedom algorithm suitable for hysteroscopic surgery which can provide stable feedback force and realistic soft body deformation effect. Our energy-based algorithm for rigid-soft body haptic interaction can synchronize iterations with different update frequency. From the experimental results, users can operate in real-time with high stability and fidelity.""","""4-DOF Haptic Rendering for Hysteroscopic Surgery Simulation""","""Haptic  Feedback""",IOxq26R8Q7A
1202,"""Lida Ghaemi Dizaji: University of Calgary; Yaoping Hu: University of Calgary; Mahdiyeh Sadat Moosavi: Arts et Metiers Institute of Technology; Frederic Merienne: Arts et Metiers Institute of Technology""","""Shape discrimination of objects relies on sensory and contextual cues. While existing studies explored cues for shape discrimination, an underexplored question remains what the minimal haptic cue (one kind of the sensory cues) is sufficient for such discrimination with contextual cues in virtual environments (VE). This study examined whether the changes of force direction – as a haptic cue – could serve this sufficiency. The results of the study confirmed the sufficiency for the discrimination under certain conditions. This confirmation implied a potential of applying force direction to simplify the design of haptic cues for VE applications.""","""Haptic Shape Perception in Virtual Environments Using Force Direction""","""Haptic  Feedback""",ZRDD5VQ2oNM
1203,"""Josué JV Vincent: ESIEA; Théo COMBE: Caplogy; Noreen Izza Arshad: Universiti Teknologi PETRONAS; Aylen Ricca: Univ. Evry, Paris-Saclay""","""This research investigates the realism of virtual touch through haptic devices, comparing high-fidelity haptic feedback (force-based) with pseudo-haptics (visual illusions of touch) in the context of training. This preliminary study involves the design and development of a virtual reality (VR) prototype, and an experimental evaluation in which participants performed an object manipulation task under two conditions (force feedback and visuo-haptics feedback) to assess weight estimation and self-performance. Results show that haptic feedback enhances realism and presence in VR, but pseudo-haptic feedback offers a cost-effective alternative for less precision-critical tasks, with potential for refinement.""","""Understanding User Perception of Haptic Illusions""","""Haptic  Feedback""",LxK0h6Fy5Dg
1204,"""Mathieu Lutfallah: ETH Zurich; Alessandro Biella: ETH Zurich; Andreas Kunz: ETH Zurich""","""This study investigates the combined effects of time pressure and interactivity on hand redirection perception in virtual reality during haptic retargeting, using an immersive shooting game as the experimental context. A total of 27 participants interacted with a haptic proxy under two experimental conditions: varying the speed of approaching enemies and the distance between the physical and virtual objects. The results revealed a high perceived normality of interactions, with over 50\% of responses rated as normal across all phases of the experiment. These findings highlight the positive influence of engagement on the perceived realism of haptic interactions.""","""Hand Redirection Thresholds in a Shooting Game""","""Haptic  Feedback""",iLfI7pfXvdo
1205,"""Sophie Villenave: Ecole Centrale de Lyon, CNRS, INSA Lyon, Universite Claude Bernard Lyon 1, Université Lumière Lyon 2, LIRIS, UMR5205, ENISE; Pierre Raimbaud: Ecole Centrale de Lyon, CNRS, INSA Lyon, Universite Claude Bernard Lyon 1, Université Lumière Lyon 2, LIRIS, UMR5205, ENISE; Guillaume Lavoué: Ecole Centrale de Lyon, CNRS, INSA Lyon, Universite Claude Bernard Lyon 1, Université Lumière Lyon 2, LIRIS, UMR5205, ENISE""","""Thermal effects in Virtual Reality (VR) can provide sensory information when interacting with objects, or create a thermal ambiance for a Virtual Environment (VE). They are critical to a range of applications, such as firefighting training or thermal comfort simulation. Existing ambient thermal feedback systems revealed limitations: some lack of proper sensory characterization, limit movements and interactions, and can be difficult to replicate. In this context, our contribution is a reproducible room-scale system able to provide dynamic ambient thermal feedback for 6 Degrees of Freedom VR experiences. We present its psychophysical study on thermal sensation, latency, and noise (n=10).""","""Dynamic and Modular Thermal Feedback for Interactive 6DoF VR""","""Haptic  Feedback""",2CtZxlhU1MA
1206,"""Kotaro Okada: Osaka University; Kazuhiro Matsui: Osaka Electro-Communication University; Ruu Fujii: Osaka University; Ryoma Kojima: Osaka University; Keita Atsuumi: Hiroshima City University; Hiroaki Hirai: Osaka University; Atsushi Nishikawa: Osaka University""","""This study explores the impact of our developed virtual self-touch (VST), where participants touch an avatar as a substitute body and receive haptic feedback, on updating the body schema for intervention in motor control strategies. VST was tested with ten participants using avatars of normal size and those with elongated forearms. The effectiveness of the method was evaluated using perceptual drift and reaching tasks, comparing VST to the arm swing task (AST). The results showed that VST enhanced perceptual drift and affected motor control strategy more than AST, suggesting its role in altering the body schema. The effect of this motor control strategy leads to rehabilitation.""","""Impact of Virtual Self-Touch on the Embodiment of Avatars with Different Body Sizes""","""Haptic  Feedback""",qO88yUAtGPk
1207,"""Yunxiu Xu: Institute of Science Tokyo; Siyu Wang: Institute of Science Tokyo; Shoichi Hasegawa: Institute of Science Tokyo""","""We introduce a lightweight wearable haptic device providing tangential force feedback through fingernail stimulation. The device uses a ring structure on the fingernail with three miniature motors: two for string-based feedback and one with an arc-shaped pin. The design maintains finger pad sensitivity by placing actuators on the nail side while delivering 2-DOF force feedback. Weighing 5.24g, the device generates nine force patterns by combining string tension and pin pressure. User studies evaluated direction discrimination and perception of weights and friction coefficients.""","""Lightweight Wearable Fingertip Haptic Device with Tangential Force Feedback based on Finger Nail Stimulation""","""Haptic  Feedback""",fs2Byr9ZsyA
1208,"""Hiroki Ota: Nara Institute of Science and Technology; Yutaro Hirao: Nara Institute of Science and Technology; Monica Perusquia-Hernandez: Nara Institute of Science and Technology; Hideaki Uchiyama: Nara Institute of Science and Technology; Kiyoshi Kiyokawa: Nara Institute of Science and Technology; Maud Marchal: Univ. Rennes, INSA, IRISA, Inria""","""This study investigates the manipulation of fingertip contact plane manipulation to enhance visuo-haptic coherency of virtual object. We designed a compact device that adjusts fingertip contact plane tilt to simulate diverse 3D shapes. Through user studies, the device demonstrated significant improvements in haptic realism, enjoyment, and operability compared to conventional methods. These findings highlight the potential of fingertip contact plane manipulation as a novel approach for advancing virtual reality (VR) haptic interfaces, enabling more immersive and realistic experiences.""","""Enhancing Visuo-Haptic Coherency by Manipulating Fingertip Contact Tilt""","""Haptic  Feedback""",yP-vO3BtxDc
1209,"""Carlos Paniagua: Nara Institute of Science and Technology; Hiroki Ota: Nara Institute of Science and Technology; Yutaro Hirao: Nara Institute of Science and Technology; Monica Perusquia-Hernandez: Nara Institute of Science and Technology; Hideaki Uchiyama: Nara Institute of Science and Technology; Kiyoshi Kiyokawa: Nara Institute of Science and Technology""","""Tape-Tics is a flexible, modular vibrotactile feedback system integrating LEDs and vibration motors. Users can adjust vibration intensity, LED colors, and configurations via Bluetooth Low Enery (BLE), enabling operation based on predefined settings. Built with flexible printed circuit (FPC) technology, Tape-Tics adapts to curved surfaces, enhancing its versatility in education, research, and art. In a hands-on workshop, participants assembled and operated Tape-Tics to explore vibrotactile technology. Using the GUI controller, they configured vibration patterns and developed unique applications. Surveys showed participants’ understanding of haptics improved significantly, and they rated Tape-Tics highly as an educational tool.""","""A Flexible Vibrotactile Feedback System for Rapid Prototyping""","""Haptic  Feedback""",rIjlwakAgLE
1210,"""Zhongyuan Yu: Technische Universität Dresden; Julian Baader: Computer Science; Matthew McGinity: Technische Universität Dresden""","""With the advent of mixed reality technologies, low-cost mobile head-mounted displays such as the Meta Quest series become widely affordable. These devices come with markerless inside-out tracking capabilities and onboard operating systems and thus can be used in large spaces for interactive data exploration. In this work, we introduce design considerations, concepts, and methods to enhance the interactive exploration and authoring of knowledge graph datasets within architectural-scale mixed reality environments. We have developed a prototype showcasing the potential of our concept. Moving forward, we plan to continue developing the system and conduct user studies to further evaluate our concepts.""","""Unfolding Complex Knowledge Graphs in Large Mixed Reality Space""","""Interaction and Interfaces""",59ttGxd049w
1211,"""Benjamin Yang: Columbia University; Jen-Shuo Liu: Columbia University; Xichen He: Columbia University; Barbara Tversky: Stanford University; Steven Feiner: Columbia University""","""Copies (proxies) of objects are useful for selecting and manipulating objects in virtual reality (VR). Temporary proxies are destroyed after use and must be recreated for reuse. Permanent proxies persist after use for easy reselection, but can cause clutter. To investigate the benefits and drawbacks of permanent and temporary proxies, we conducted a user study in which participants performed 6DoF tasks with proxies in the Voodoo Dolls technique, revealing that permanent proxies were more efficient for hard reselection and preferred.""","""Permanent Proxies for Selection and Manipulation in Virtual Reality""","""Interaction and Interfaces""",TRDW5mpFBng
1212,"""Jalal Safari Bazargani: Sejong University; Soo-Mi Choi: Sejong University""","""This paper presents a multi-modal human-agent interaction framework that integrates hand tracking, voice commands, user intention interpretation, and scene understanding using large language and vision models for object pick-up and placement tasks in VR. Scene understanding integrates object tags and ChatGPT-4V's object detection. User commands are processed via the LLaMA-3.2 model, which interprets user intention, identifies task and reference objects, and spatial relationships for task execution. An initial experiment with 15 participants showed that the multimodal approach achieved the fastest task completion times and highest user satisfaction. While hand-only interaction was comparable, voice-only interaction was significantly slower.""","""Multi-modal Interaction with Virtual Agents: A Pick-up and Placement Case Study""","""Interaction and Interfaces""",gncR4ZXXRNg
1213,"""Melissa Steininger: University Hospital Bonn; Anna Jansen: University Hospital Bonn; Kristian Welle: University Hospital Bonn; Björn Krüger: University Hospital Bonn""","""Hand tracking plays an important role in many Virtual Reality (VR) applications, enabling natural user interactions. Achieving precise tracking is often challenged by occlusion and suboptimal sensor placement. To address these challenges, we developed the Sensor Positioning Simulator, a versatile tool designed to optimize sensor placement. To demonstrate its utility, we simulated scenes from the VIRTOSHA project, a VR-based surgical training platform. Evaluations show that the tool effectively positioned sensors to achieve maximum hand surface visibility and full hand movement area coverage, even in occlusion-heavy environments. Future developments include support for animated simulations and validation through real-world experiments.""","""Optimized Sensor Position Detection: Improving Visual Sensor Setups for Hand Tracking in VR""","""Interaction and Interfaces""",Am0NbE0f_lk
1214,"""Zihao Li: Beijing Engineering Research Center of Mixed Reality and Advanced Display; Dongdong Weng: Beijing Engineering Research Center of Mixed Reality and Advanced Display,School of Optics and Photonics; Xiaonuo Dongye: Beijing Engineering Research Center of Mixed Reality and Advanced Display; Jie Hao: Beijing Engineering Research Center of Mixed Reality and Advanced Display; Xiangyu Qi: Beijing Engineering Research Center of Mixed Reality and Advanced Display""","""Text entry in virtual reality (VR) is a common task; however, it is often hindered by low typing efficiency and physical fatigue. To address these challenges, we present OrbitText, a novel text entry system for VR environments. OrbitText features a dual-handed, minimal-motion 3D interface and integrates an innovative typing prediction model that combines large language models (LLMs) with n-gram models. Our design aims to minimize physical strain while enhancing typing efficiency. User studies demonstrate that OrbitText significantly outperforms mainstream virtual keyboards, achieving higher efficiency, reduced fatigue, and a lower perceived task load.""","""OrbitText A Hybrid Prediction System for Efficient and Effortless Text Entry in Virtual Reality""","""Interaction and Interfaces""",DqcSK_FUVx4
1215,"""Taewook Ha: KAIST; Selin Choi: KAIST; Seonji Kim: KAIST; Dooyoung Kim: KAIST; Woontack Woo: KAIST""","""We present a new approach using Virtual Reality to capture human-scene interaction data within 3D environments. Traditional methods required costly physical setups with real furniture, making it difficult to collect both interaction and 3D scene data comprehensively. Our solution implements a user-centric scene graph with a hierarchical structure that captures both user interactions and 3D scene elements. The resulting dataset contains real human interaction data, dynamic scene information, synthetic sensor data, and scene graph annotations. This work provides a foundation for developing Mixed Reality experiences using 3D scene graphs with human interaction data.""","""Human-Scene Interaction Data Generation with Virtual Environment using User-Centric Scene Graph""","""Interaction and Interfaces""",DOG8t2Qi0FI
1216,"""Matthieu Blanchard: Ecole Centrale de Lyon, CNRS, INSA Lyon, Universite Claude Bernard Lyon 1, Université Lumière Lyon 2, LIRIS, UMR5205, ENISE; Sophie Villenave: Ecole Centrale de Lyon, CNRS, INSA Lyon, Universite Claude Bernard Lyon 1, Université Lumière Lyon 2, LIRIS, UMR5205, ENISE; Tristan Habémont: INSA Lyon, Ecole Centrale Lyon, Universite Claude Bernard Lyon 1, CPE Lyon, CNRS, INL UMR 5270; Bertrand Massot: INSA Lyon, Ecole Centrale Lyon, Universite Claude Bernard Lyon 1, CPE Lyon, CNRS, INL UMR 5270; Pierre Raimbaud: Ecole Centrale de Lyon, CNRS, INSA Lyon, Universite Claude Bernard Lyon 1, Université Lumière Lyon 2, LIRIS, UMR5205, ENISE; Guillaume Lavoué: Ecole Centrale de Lyon, CNRS, INSA Lyon, Universite Claude Bernard Lyon 1, Université Lumière Lyon 2, LIRIS, UMR5205, ENISE""","""Physiological data are increasingly used in Virtual Reality (VR) to analyze the user experience. However, when we move and interact in VR, the quality of physiological data is degraded. This study evaluates the impact of movements and sensor positions on signal quality. 3 positions for EDA and 2 for ECG have been compared on 36 subjects performing a series of different gestures in a within-subjects design. We observed that movements requiring the hand to be closed significantly degraded EDA signals, and that the palm position offered better robustness. For ECG, the position closest to the heart gave the best results.""","""Physiological Signals Quality in 6DoF Virtual Reality: Preliminary Results""","""Interaction and Interfaces""",CAyig-Txpf8
1217,"""Nicolas LEVI-VALENSI: Aix-Marseille Université; Antoine HP MORICE: Aix-Marseille Université; William WILMOT: Stellantis; Jocelyn Monnoyer: Stellantis; Dr Julien MAROT: Aix-Marseille Université""","""We investigated how to design a software pipeline to recognize hand gesture from images inside a vehicle cockpit. Two architectures are compared. Frequency descriptors derived from the Fourier transforms were solely used in image outline analysis.""","""Autonomous Hand Gesture Recognition Solution for Embedded Automotive Systems""","""Interaction and Interfaces""",h4UU-NK0ZmA
1218,"""Marine Desvergnes: Université de Poitiers; Cecile R. Scotto: Université de Poitiers; Centre de Recherches sur la Cognition et l'Apprentissage; Kathleen Belhassein: CNRS; Célestin Preault: CESI Lineact; Jean JP Pylouster: Université de Poitiers; Pierre Laguillaumie: CNRS; Jean-Pierre Gazeau: Université de Poitiers - CNRS; Ludovic Le Bigot: Université de Poitiers; Nicolas Louveton: Université de Poitiers""","""Immersive technologies enhance industrial applications by creating virtual representations of manufacturing environments and providing visual assistance for tasks. Most studies are based on ad hoc scenarios, making comparisons across studies difficult. In this research, 99 participants controlled a remote industrial robot using a VR headset in a reproducible maintenance task. They performed the task under three conditions: no assistance (control), text-based assistance, or attentional cueing (highlighting objects). We manipulated task difficulty (easy vs. difficult) and assessed performance based on completion time and failure rate. Results showed that visual assistance significantly shortened task completion time, with findings discussed.""","""Visual assistance in VR-based robot control : towards a reproducible evaluation scenario""","""Interaction and Interfaces""",hS6Yi8OUPCA
1219,"""Kagan Taylor: Lancaster University; Haopeng Wang: Lancaster University; Florian Weidner: Lancaster University; Hans Gellersen: Lancaster University""","""Gaze and hand-based micro-gestures have been widely studied individually, each offering unique advantages as input modalities, but their combination remains under-explored. In this work, we integrate gaze with thumb-to-finger micro-gestures, discussing their complementary strengths to enable expressive and efficient interactions. We examine two strategies for combining gaze and micro-gestures, assess their strengths and weaknesses, and present examples such as mouse emulation and application shortcuts. Finally, we outline the potential and challenges of gaze and micro-gestures to enhance interaction across diverse contexts, and discuss future directions. This work begins the exploration of gaze and micro-gestures for novel input techniques.""","""Towards the Fusion of Gaze and Micro-Gestures""","""Interaction and Interfaces""",g-1SkQSMOY0
1220,"""Kazuma Mori: Nara Institute of Science and Technology; Isidro M. Butaslac III: Nara Institute of Science and Technology; Taishi Sawabe: Nara Institute of Science and Technology; Hirokazu Kato: Nara Institute of Science and Technology; Alexander Plopski: TU Graz""","""Automatic AR-based work support systems promise to provide instructions without user interaction when the current step is completed. However, progress estimation failures can cause confusion and reduce trust. To support understanding of instruction reliability, we display the confidence of each step. To explore its effects on user interaction, we conducted a Wizard-of-Oz style user study with 12 participants assembling a LEGO model with and without confidence visualization, where confidence levels were predefined. While the visualization did not improve performance, participants preferred having it, especially for low-confidence steps. These findings show design possibilities for future automated instruction systems.""","""Effects of Confidence Visualization in Automated Assembly Instructions""","""Interaction and Interfaces""",MSGNkXpscqY
1221,"""Polina Häfner: Karlsruhe Insitute of Technology; Frithjof Anton Ingemar Eisenlohr: Karlsruhe Insitute of Technology; Felix Longge Michels: Karlsruhe Insitute of Technology; Abhijit Karande: EES Beratungsgesellschaft mbH; Michael Grethler: EES Beratungsgesellschaft mbH""","""This paper proposes a conceptual framework leveraging VR technologies to develop large language model (LLM)-based Voice User Interfaces (VUIs) for industrial applications, addressing key adoption challenges. By integrating virtual twins and immersive environments, the framework supports essential development steps, including data collection, LLM training, usability testing, and iterative design validation. The study shows how VR enables realistic simulations, faster development, and reduces the dependency on physical prototypes in the VUI development, offering improvements in efficiency and user-centered design. Initial findings of the proof-of-concept study highlight the framework's potential to address industrial requirements.""","""Accelerating the Development of Machine Voice User Interfaces with Immersive Environments""","""Interaction and Interfaces""",XTg7R5c1HqM
1222,"""Francesco Vona: University of Applied Sciences Hamm-Lippstadt; Julia Schorlemmer: Immersive Reality Lab, University of Applied Sciences Hamm-Lippstadt ; Jessica Stemann: University of Applied Sciences Hamm-Lippstadt; Sebastian Fischer: University of Applied Sciences Hamm-Lippstadt; Jan-Niklas Voigt-Antons: Hamm-Lippstadt University of Applied Sciences""","""Virtual reality (VR) enables users to simulate real-life situations in immersive environments. Interaction methods significantly shape user experience, particularly in high-fidelity simulations mimicking real-world tasks. This study evaluates two primary VR interaction techniques—hand-based and controller-based—through virtual shopping tasks in a simulated supermarket with 40 participants. Hand-based interaction was preferred for its natural, immersive qualities and alignment with real-world gestures but faced usability challenges, including limited haptic feedback and grasping inefficiencies. In contrast, controller-based interaction offered greater precision and reliability, making it more suitable for tasks requiring fine motor skills.""","""Hands vs. Controllers: Comparing User Interactions in Virtual Reality Shopping Environments""","""Interaction and Interfaces""",aBjL4mvtqyw
1223,"""Hong Son Nguyen: Korea University; Andrew Chalmers: Victoria University of Wellington; DaEun Cheong: Korea University; Myoung Gon Kim: Korea University; Taehyun James Rhee: University of Melbourne; JungHyun Han: Korea University""","""This paper presents a pipeline that estimates a user's 3D pose and shape to facilitate a user’s full-body interaction with virtual objects in a mixed reality environment. The usability and effectiveness of the pipeline are demonstrated through a user study.""","""Full-Body Interaction in Mixed Reality using 3D Pose and Shape Estimation""","""Interaction and Interfaces""",6CT28Q_SnZ8
1224,"""Alexandre Vu: M2S - EA7470, Univ Rennes 2, Inria; Anthony Sorel: M2S - EA7470, Univ Rennes 2, Inria; Benoit Bideau: M2S - EA7470, Univ Rennes 2, Inria; Richard Kulpa: M2S - EA7470, Univ Rennes 2, Inria""","""The multitasking aspect of soccer is rarely considered to study the relationship between expertise and visual attention. This study examined the ability of 24 soccer players to visually track teammates and opponents in simulated game situations within a CAVE, with or without the requirement to intercept a virtual ball. No significant decrease in visual tracking performance was observed when an actual motor interception was required. Additionally, no significant differences were found between participants competing at the national level and those at lower levels. Further research is needed to better understand how visual attention contributes to expertise in soccer.""","""Influence of ball reception on visual tracking performance of soccer players in a virtual reality Multiple Player Tracking task""","""Interaction and Interfaces""",Wyac3QR_0WI
1225,"""Zachary C Jones: Concordia University; Simon Drouin: École de Technologie Supérieure; Marta Kersten-Oertel: Concordia University""","""Virtual reality (VR) facilitates immersive visualization and interaction with complex medical images in immersive platforms. It remains unclear which VR input schemes and devices are optimal for these tasks. In this study, we perform a 12-person study to investigate user performance and experience while performing medical image segmentation with two control schemes, keyboard and mouse (KBM) and VR motion controllers (MC). Our results showed that motion controllers are faster in smaller segmentation tasks and offer a more pleasant user experience, however no significant difference in user accuracy was observed.""","""Exploring Interaction Paradigms for Performing Medical Image Segmentation Tasks in Virtual Reality""","""Interaction and Interfaces""",uE7AMdRkBuM
1226,"""Ryutaro Kurai: Cluster, Inc.; Takefumi Hiraki: Cluster Metaverse Lab; Yuichi Hiroi: Cluster Metaverse Lab; Yutaro Hirao: Nara Institute of Science and Technology; Monica Perusquia-Hernandez: Nara Institute of Science and Technology; Hideaki Uchiyama: Nara Institute of Science and Technology; Kiyoshi Kiyokawa: Nara Institute of Science and Technology""","""Metaverse platforms are rapidly evolving to provide immersive spaces. However, the generation of dynamic and interactive 3D objects remains challenging due to the need for advanced 3D modeling and programming skills. We present MagicCraft, a system that generates functional 3D objects from natural language prompts. MagicCraft uses generative AI models to manage the entire content creation pipeline: converting user text descriptions into images, transforming images into 3D models, predicting object behavior, and assigning necessary attributes and scripts. It also provides an interactive interface for users to refine generated objects by adjusting features like orientation, scale, seating positions, and grip points.""","""An implementation of MagicCraft: Generating Interactive 3D Objects and Their Behaviors from Text for Commercial Metaverse Platforms""","""Interaction and Interfaces""",NtsWOCxC0x0
1227,"""Disha Sardana: Virginia Tech; Lee Lisle: Virginia Tech; Denis Gracanin: Virginia Tech; Ivica Ico Bukvic: Virginia Tech; Kresimir Matkovic: VRVis Research Center; Greg Earle: Virginia Tech""","""This study evaluates the role of sonification in immersive analytics using real-world geophysical datasets. A between-subject experiment in a mixed-reality environment compared audio-visual and visual-only scenarios with 50 participants. The study analyzed key performance metrics, including pattern identification, confidence, task responses, workload (NASA Task Load Index), and usability (SUS questionnaire). Results show that sonification enhances pattern detection and user confidence in analytics tasks. This research highlights sonification's benefits and limitations, offering insights into optimizing audio for immersive systems to support complex data exploration.""","""Evaluating the Impact of Sonification in an Immersive Analytics Environment Using Real-World Geophysical Datasets""","""Interaction and Interfaces""",I5uS5VoExmM
1228,"""Alexander Gao: University of Maryland, College Park; Xijun Wang: The University of Manyland, College Park; Geonsun Lee: University of Maryland; William Barber Smith Chambers: University of Maryland; Niall L. Williams: University of Maryland, College Park; Yi-Ling Qiao: University of Maryland, College Park; Shengjie Xu: University of Maryland, College Park; Ming C Lin: University of Maryland at College Park""","""In immersive VR environments, users often miss events occurring outside their field of view. To address this, we propose a framework that guides user attention by dynamically adjusting environment lighting based on an input text script and 3D scene.   Validated through a user study, our system improves users' recall of virtual environments.""","""Event-Driven Lighting for Immersive Attention Guidance""","""Interaction and Interfaces""",fZspvtg9GkA
1229,"""Zhaomou Song: University of Cambridge; John J Dudley: University of Cambridge; Per Ola Kristensson: University of Cambridge""","""Gesture interaction systems continuously predict user gestural inputs to allow rapid access to functions. It is essential to provide additional assistance when the system makes a mistake. We introduce two alternative assistance strategies, a contextual menu and visual-based guidance, that prompt the user with additional information when the system uncertainty exceeds a threshold. We compared the proposed approaches with a fixed menu in a user study with 18 participants. The results suggest that participants universally praised the fixed menu for its high controllability. Visual guidance was also preferred by many participants due to its simplicity and intuitiveness, especially for those who seek understanding about the system.""","""Investigating Visualization and Control for Assisting Gesture Interaction in Virtual Reality""","""Interaction and Interfaces""",S2W3Y3iUxis
1230,"""Aleshia Hayes: University of North Texas; Tania Heap: University of North Texas; Deborah Cockerham: University of North Texas; Lauren Eutsler: University of North Texas; Ruth West: University of North Texas; Haseeb Abdul: University of North Texas; Erika Knapp: University of North Texas; Neureka Kandu: University of North Texas""","""Highly immersive, interactive VR is engaging and effective for education and training across users, contexts, and domains. However, VR tools are not recommended for early elementary school children. Here we report on the participatory design approach to three novel mixed reality displays and a mixed reality software, “The Readerverse,” in the context of reading proficiency to demonstrate application of the affordances of mixed reality for early elementary education. This mixed reality library allows students to open virtual books that transport them into the book world with content-specific learning experiences. This paper discusses pilot testing and iterative design with stakeholders.""","""Designing a Mixed Reality Library Experience to Engage Students with Open-Source Literacy Content for Primary School Students""","""Interaction and Interfaces""",Uab9QlYsKno
1231,"""Chong CHENG: Hong Kong University of Science and Technology (Guangzhou); Qinzheng Zhou: National Key Laboratory of Multispectral Information Processing; Jianfeng Zhang: NUS; Shiya Tsang: The Hong Kong University of Science and Technology; Hao Wang: HKUST""","""This paper presents Decoupled3D, a novel framework for 3D object generation with decoupled components. Decoupled3D uniquely decouples and optimizes object component meshes from a single image, producing not only an overall shape for the object but also the independent modeling for object components. Decoupled3D serves as a plug-and-play module, which is applicable to various object images and existing 3D reconstruction methods, offering high flexibility and scalability. Experimental results suggest the potential in digital creation and virtual reality applications. The survey results indicate that participants believe our proposed Decoupled3D offers superior control and design capabilities.""","""Exploring Decoupled Generation for Enhancing VR Interaction and Control""","""Miscellaneous Topics""",TfpMj_UG9Ak
1232,"""Victor Häfner: Karlsruhe Insitute of Technology; Polina Häfner: Karlsruhe Insitute of Technology; Felix Longge Michels: Karlsruhe Insitute of Technology; Florian Bauer: Karlsruhe Insitute of Technology; Michael Grethler: Karlsruhe Insitute of Technology""","""Geothermal energy offers a sustainable and reliable energy source. However, VR engineering methods have seen limited application in this field. This work introduces an Immersive Engineering Toolkit, which leverages an open-source VR engine to address challenges across the geothermal lifecycle, such as drill site planning, subsurface visualization, engineering simulations, and public engagement. By integrating heterogeneous data formats and enabling collaborative environments, the toolkit aims to enhance cost-efficiency, safety, decision-making and stakeholder communication. By unifying scientific, engineering, and societal aspects, the toolkit aims to provide a holistic approach to accelerate the adoption of deep geothermal energy systems.""","""Immersive Engineering Toolkit for Deep Geothermal Energy""","""Miscellaneous Topics""",8wLMQJbwc9Q
1233,"""Arwa Own: Chemnitz University of Technology; Sebastian Knopp: Chemnitz University of Technology; Mario Lorenz: Chemnitz University of Technology""","""Using Artificial Intelligence (AI) to validate that tasks are performed correctly when instructed through Augmented Reality (AR) is a rather unexplored area. Focusing on the removal of components during the disassembly of washing machines in the context of circular economy, we develop an AR system, to detect correctly executed tasks using AI. We present preliminary results of our novel approach using the YOLOv8 Nano model detecting eight components. Already with very limited training data we could achieve detection precisions of over 90% for seven of eight components proofing the general feasibility of our work.""","""Industrial Augmented Reality – Feasibility of AI-based Validation of AR Instructed Component Recovery for Washing Machines in Circular Economy""","""Miscellaneous Topics""",LWuRzmSLRn0
1234,"""Masaki Yoshida: Hokkaido University; Ren Togo: Hokkaido University; Takahiro Ogawa: Hokkaido University; Miki Haseyama: Hokkaido University""","""This paper proposes a novel method to extend Gaussian Splatting (3DGS) to the audio domain, enabling novel-view acoustic synthesis solely using audio data. While recent advancements in 3DGS have significantly improved novel-view synthesis in the visual domain, its application to audio has been overlooked, despite the critical role of spatial audio for immersive AR/VR experiences. Our method addresses this gap by constructing an audio point cloud from audio at source viewpoints and rendering spatial audio at arbitrary viewpoints. Experimental results show that our method outperforms existing approaches relying on audio-visual information, demonstrating the feasibility of extending 3DGS to audio.""","""Extending Gaussian Splatting to Audio: Optimizing Audio Points for Novel-view Acoustic Synthesis""","""Miscellaneous Topics""",LuYWZUFFBxI
1235,"""Nina Rosa: Wageningen University and Research; Esther Kok: Wageningen University and Research; Els Siebelink: Wageningen University and Research; Michelle van Alst: Wageningen University and Research; Travis Masterson: Pennsylvania State University; Alexander Klippel: Wageningen University and Research""","""Portion size estimation is an essential component of dietary intake assessment, but people generally struggle with such estimation. Augmented reality methods have been developed to aid in this, but it is unclear how selected features are intended to improve accuracy or create a positive user experience. In this paper, we present a storyboard for a new augmented reality approach, AReplica. We explain why each feature was chosen and list necessary future research.""","""AReplica: User-Centered Design of an Augmented Reality Portion Size Estimation tool for Dietary Intake Assessment""","""Miscellaneous Topics""",djyZUII5I3Q
1236,"""Nanjia Wang: University of Calgary; Muskan Sarvesh: University of Calgary; Brody Wells: University of Calgary; Bryson Lawton: University of Calgary; minseok ryan kang: University of Calgary; Kangsoo Kim: University of Calgary; Frank Maurer: University of Calgary""","""This paper presents an immersive extended reality (XR) based digital twin (DT) system designed for a hydrogen pipeline test facility, combining real-time data visualization with interactive virtual navigation. Employing 3D pipeline models and a point-and-teleport locomotion method with orientation specifications, the system enables efficient exploration of the physical pipeline facility's virtual counterpart. Preliminary feedback from field experts highlights the system's potential to provide effective navigation and reduce cognitive fatigue through the visualization of the sensor data.""","""Developing an XR-Integrated Digital Twin for Hydrogen Pipeline Monitoring and Navigation""","""Miscellaneous Topics""",UcdgwNi9YDw
1237,"""Yanan Liu: Yunnan university; Yanqiu Li: The University of Melbourne; Hao Zhang: Yunnan University; Qianhan Tang: Yunnan University; Dan Xu: Yunnan University""","""Human action recognition has vast research prospects in the field of virtual reality, particularly in the context of the skeleton modality with enhanced robustness to background noise.  However, recent methods overlook a dataset bias caused by  the customized and subject-specific  sub-actions  correlated spurious causalities between  actions and predicted labels.  In this work, we build a skeleton causal graph  to formulate the causalities and causally intervened the confounded recognition model via backdoor adjustment, and design  a novel causal intervention based pipeline CI-GCN for skeleton-based action recognition to deconfound model and improve recognition accuracy, which achieves the state-of-the-art on three public action datasets.""","""Deconfounded Human Skeleton-based Action Recognition via Causal Intervention""","""Miscellaneous Topics""",n7VpZ2VsHQE
1238,"""Nader Alfares: Pennsylvania State University; George Kesidis: Pennsylvania State University""","""We introduce a Unity based benchmark for evaluating Virtual Reality (VR) delivery systems using edge-cloud caching. As VR evolves, meeting strict latency and Quality of Experience (QoE) requirements is critical. Traditional cloud architectures often struggle to meet these demands. With edge computing, resources are brought closer to users in efforts to reduce latency and improve QoEs. However, challenges arise from changing fields of view (FoVs) and synchronization requirements. We address the lack of suitable benchmarks and propose a framework that simulates multiuser VR scenarios while logging users' interaction with objects within their FoVs, supporting research in optimizing edge caching and other edge-cloud functions for VR streaming.""","""Virtual Reality Benchmark for Edge Caching Systems""","""Miscellaneous Topics""",olmV0MXQ4zg
1239,"""Alexandre Courallet: CESI; David Baudry: CESI; Vincent Havard: CESI""","""Extended Reality-Interfaced Digital Twins (XR-DTs) can be used in a wide range of industrial applications, including training, simulation, and the design and monitoring of industrial systems. However, interacting with and controlling autonomous agents, whether virtual or real, through XR-DT can be challenging, and existing solutions often address only a certain level of collaboration or limited use cases. To address these limitations, this paper presents a no-code scenario authoring solution for XR-DTs called NEURONES as well as a demonstration scenario.""","""Authoring framework for industrial XR digital twin and autonomous agent : a proof of concept""","""Miscellaneous Topics""",W928FWK4Lnk
1240,"""Mohammad Jahed Murad Sunny: University of Arkansas at Little Rock; Jan P Springer: University of Arkansas at Little Rock; Aryabrata Basu: University of Arkansas at Little Rock""","""Our study explores the relationship between VR experience, 3D gaming expertise, and performance metrics like task completion time, accuracy, and workload. Analysis reveals that combined expertise in VR and 3D gaming improves efficiency, reduces workload, and enhances accuracy. Notably, users with multidimensional experience consistently perform better with higher accuracy and lower task-load scores, demonstrating the value of VR proficiency. Interestingly, physical space requirements remain low across all levels, highlighting VR’s accessibility.""","""Evaluating Adjustment and Proficiency Disparities in Virtual Reality""","""Miscellaneous Topics""",Gqg5rxaCWeU
1241,"""Xinyu Liu: King Abdullah University of Science and Technology; Dawar Khan: King Abdullah University of Science and Technology; Omar Mena: King Abdullah University of Science and Technology; Donggang Jia: King Abdullah University of Science and Technology (KAUST); Alexandre Kouyoumdjian: King Abdullah University of Science and Technology; Ivan Viola: King Abdullah University of Science and Technology""","""We evaluate 17 LLMs across four XR devices—Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro—assessing performance on key metrics: consistency, processing speed,  and battery consumption. Our experimental setup examines 68 model-device pairs under varying string lengths, batch sizes, and thread counts, providing insights into trade-offs for real-time XR applications. Our findings offer guidance for optimizing LLM deployment on XR devices and establish a foundation for future research in this rapidly evolving field.""","""LLMs on XR (LoXR): Performance Evaluation of LLMs Executed Locally on Extended Reality Devices""","""Miscellaneous Topics""",LjjO9nvOpF4
1242,"""Alex Orsholits: The University of Tokyo; Manabu Tsukada: The University of Tokyo""","""Spatial computing is evolving towards leveraging data streaming for computationally demanding applications, facilitating a shift to lightweight, untethered, and standalone devices. These devices are ideal candidates for co-processing, where real-time scene context understanding and low-latency data streaming are fundamental for general-purpose Mixed Reality (MR) experiences. This poster demonstrates and evaluates a scalable approach to augmented contextual understanding in MR by implementing edge AI co-processing through a Hailo-8 AI accelerator, a low-power ARM-based single board computer (SBC), and the Magic Leap 2 AR headset. The resulting inferences are streamed back to the headset for spatial reprojection into the user’s vision.""","""Edge Vision AI Co-Processing for Dynamic Context Awareness in Mixed Reality""","""Miscellaneous Topics""",xxahKZl4K9w
1243,"""Matthew Zhang: Rhodes College; Nate Phillips: Rhodes College""","""Depth perception in AR describes the ability to perceive depth from AR-generated objects. This functionality enables an emergent capability: x-ray vision, or visualizing objects past an occluding surface. To evaluate x-ray vision’s feasibility, we propose experiments that depict a virtual object beyond a solid wall. In one condition this display is mitigated with a semi-transparent virtual window, while in another condition no such effect is presented. Depth estimates will be measured using triangulation by walking. Our motivation is to understand discrepancies between perceived distances of virtual and real-world objects so that AR systems can accurately display objects in the real-world.""","""Evaluating Depth Perception in Augmented Reality X-Ray Vision""","""Perception and Cognition""",MFhtltX-4zg
1244,"""Agata Szymańska: Jagiellonian University; Paweł Jemioło: AGH University of Krakow; Beata Pacula-Leśniak: Jagiellonian University; Michał Kuniecki: Institute of Psychology, Faculty of Philosophy, Jagiellonian University in Krakow""","""Adapting VR scenarios in real time based on users’ emotional responses is a promising advancement for developers. However, most VR environments are either ""blind"" to emotions or depend on intrusive, contact-based methods. This study introduces a novel, non-invasive approach for predicting arousal and valence using pupil reactivity and gaze features. Data from 105 participants (ages 18–30) viewing 120 emotional images via a VR headset with eye tracking informed machine learning models. These models, leveraging features like mean and standard deviation of pupil data, achieved F1 scores of 0.64 (valence) and 0.73 (arousal), highlighting potential real-world applications in VR development.""","""Pupil-Based Prediction of Valence and Arousal in VR: A Machine Learning Approach""","""Perception and Cognition""",dGUM7plE9gw
1245,"""Beatrice Biancardi: CESI LINEACT; Mukesh Barange: CESI LINEACT; Matthieu Vigilant: Université Paris Cité; Pierre-Louis Lefour: Université Paris Cité; Laurence Chaby: Université Paris Cité""","""First impressions are critical in shaping social interactions, with perceptions of interpersonal warmth playing a central role in fostering positive judgments. Social touch, such as handshake, conveys both interpersonal and physical warmth, potentially influencing impressions and social proximity. Using VR and haptic technologies, we explore how  handshake temperature (warm or cold) affects first impressions, and interpersonal distance, during interactions with a virtual agent. We aim to discuss the implications for designing more engaging and realistic human-agent interactions, with a focus on the role of haptic feedback in fostering positive impressions.""","""Warming the Ice: The Role of Social Touch and Physical Warmth on First Impressions in Virtual Reality""","""Perception and Cognition""",8CCQks15gA8
1246,"""Lena Holderrieth: University of Würzburg; Erik Wolf: University of Würzburg; Marie Luisa Fiedler: University of Würzburg; Mario Botsch: TU Dortmund University; Marc Erich Latoschik: University of Würzburg; Carolin Wienrich: University of Würzburg""","""Body weight issues can manifest in low self-esteem through a negative body image or the feeling of unattractiveness. To explore potential interventions, the pilot study examined whether embodying a photorealistically personalized avatar with enhanced attractiveness affects self-esteem. Participants in the manipulation group adjusted their avatar's body weight to their self-defined ideal, while a control group used unmodified avatars. To confirm the manipulation, we measured the perceived avatars' attractiveness. Results showed that participants found avatars at their ideal weight significantly more attractive, confirming an effective manipulation. Further, the ideal weight group showed a clear trend towards higher self-esteem post-exposure.""","""Do You Feel Better? The Impact of Embodying Photorealistic Avatars with Ideal Body Weight on Attractiveness and Self-Esteem in Virtual Reality""","""Perception and Cognition""",D3MY450oVac
1247,"""Leon Lange: UC San Diego; Jacob Yenney: UC San Diego; Ying Choon Wu: UC San Diego""","""This study explores approaches to combining real-time electroencephalographic (EEG) with VR to generate synthetic psychedelic experiences. Using a passive brain-computer interface (BCI) that dynamically adjusts virtual content, we aim to elicit neurocognitive states similar to those that characterize psychedelic trips - including attenuation of self-referential thought and diminished engagement of the default mode network. Preliminary results indicate that this type of personalized VR experience can modulate cortical activities and elicit unique states of altered consciousness. By mimicking hallucinatory experiences, we aim in the future to replicate the cognitive and emotional benefits of psychedelic agents without the associated risks.""","""Dynamic VR Modulation with EEG Feedback for Psychedelic Simulation""","""Perception and Cognition""",EXxg_MRUikI
1248,"""Michèle Atié: Nantes Université, ENSA Nantes, Ecole Centrale Nantes, CNRS, AAU-CRENAU, UMR 1563; Céline Drozd: Nantes Université, ENSA Nantes, Ecole Centrale Nantes, CNRS, AAU-CRENAU, UMR 1563; Toinon Vigier: Université de Nantes; Daniel Siret: Nantes Université, ENSA Nantes, Ecole Centrale Nantes, CNRS, AAU-CRENAU, UMR 1563""","""This paper studies the potential of VR HMDs to replicate subjective impressions of luminous atmospheres using 360° photographs. We compare the perception of luminous atmospheres in two settings—VR HMD and UHD screen—against real iconic buildings. Evaluations were conducted through three distinct experiments: in seven real indoors of iconic buildings, using 360° photographs of these indoors viewed in a VR HMD, and using 2D images from these indoors displayed on a UHD screen. Results indicate that VR HMDs are a promising tool for studying luminous atmospheres, providing a closer perception to real-world conditions.""","""Luminous Atmospheres 360: Comparing VR HMDs and UHD Screens for Reproducing Architectural Daylight Atmospheres""","""Perception and Cognition""",TFDcuJnAfmc
1249,"""Antoine HP MORICE: Aix-Marseille Université; Nicolas SALVAGE: Aix-Marseille Université; Emilie DUBUISSON: Aix-Marseille Université""","""The automotive industry faces the challenge of determining whether Mixed Reality displays (MR), which are expensive, outperform traditional head-down displays in depicting Advanced Driver Assistant Systems (ADAS). We designed a novel ecological interface for an ADAS to enhance drivers’ perception of Turnability - an affordance specifying the possibility of crossing a narrow orthogonal alleyway following a forward constant-curvature turn. We augmented the perception of turnability with configural (head-down) and conformal (MR) displays. Overall, both displays improved drivers’ decision-making. However, the configural display is not only superior but also rated as more usable than the conformal display by the drivers.""","""Effectiveness of MR to enhance drivers’ perception of turnability""","""Perception and Cognition""",LlJNzbmqa3M
1250,"""Jorge Pina: Universidad de Zaragoza; Edurne Bernal-Berdun: Universidad de Zaragoza - I3A; Daniel Martin: Universidad de Zaragoza; Sandra Malpica: Centro Universitario de la Defensa, I3A; Carmen Real: Universidad de Zaragoza; Pablo Armañac: Universidad de Zaragoza; Jesus Lazaro: Universidad de Zaragoza; Alba Martin: Universidad de Zaragoza; Belen Masia: Universidad de Zaragoza; Ana Serrano: Universidad de Zaragoza""","""Cognitive load, the mental effort required to process information and perform tasks, affects user performance and engagement. Understanding its impact in immersive, multisensory experiences is critical for task-oriented applications like training and education. We studied the effect of cognitive load on user performance on a visual search task, performed on its own or alongside a secondary, auditory task. This effect was tested for two different search areas: 90º and 360º. Results show a decline of task performance with increased cognitive load, but with different trends between the 90º and 360º cases.""","""The Effect of Cognitive Load on Visual Search Tasks in Multisensory Immersive Environments""","""Perception and Cognition""",8zSuL5SFKGI
1251,"""Delphine Yeh: Université Paris Cité; Sylvain Penaud: Université Paris Cité; Alexandre Gaston-Bellegarde: Université Paris Cité; Pascale Piolino: Université Paris Cité""","""The self-reference effect (SRE) suggests that encoding new material in episodic memory (EM) is enhanced when information is closely linked to the Self. However, traditional approaches to the SRE have neglected the interplay between multiple facets of the Self and naturalistic, multisensory EM contexts. The present study investigates the respective and joint contributions of the minimal and narrative Selves to the SRE during the encoding of naturalistic, multisensory daily events in a virtual city, experienced while embodied in a first-person avatar. Preliminary findings suggest that the narrative Self plays a predominant role in enhancing EM encoding under ecological conditions.""","""Encoding Multimodal Scenes in a Virtual City: How the Multifaceted Self Shapes Naturalistic Episodic Memory""","""Perception and Cognition""",U6844z-HTMA
1252,"""Yobbahim J. Vite: University of Calgary; Yaoping Hu: University of Calgary""","""To enable adaptive virtual reality (VR) based systems, this study explored to differentiate attention’s 3 substates – i.e., orienting (OR), alerting (AL), and cognitive maintenance (CM). Human participants performed a task within a VR-based cockpit to evoke these substates, while their brain activity was recorded as electroencephalographic data to extract features. Statistical analyses revealed significant differences of these features among the substates. Compared 5 machine-learning models, naïve bayes and K-nearest neighbors achieved similarly the best accuracy (77% ~ 95%) to classify the features. These findings implied a potential of the features for informing design of adaptive VR-based systems.""","""Attention's Substates: Unlocking Adaptive VR through EEG Insights""","""Perception and Cognition""",H7mwp2ljyVE
1253,"""Stanley Tarng: University of Calgary; Yaoping Hu: University of Calgary""","""In this study, we investigated the relationship between behavioral and brain responses during haptic interactions in a virtual environment. Using a helicopter aerial firefighting simulation, ten participants interacted with co-located and dis-located haptic cues. Behavioral data, including reaction times were analyzed using Drift-Diffusion Model (DDM) along event-related potentials, focusing on N200 and P300 features. Bland-Altman analyses revealed potential agreement between DDM drift rate and ERP feature slopes, suggesting that internal cognitive processes can be inferred behaviorally. The results indicate that behavioral models, such as DDM, may reduce the need for direct brain measurements, with implications for haptic interface design.""","""Brain Activity and Decision-Making with Haptic Cues""","""Perception and Cognition""",EJIV0CuGiTM
1254,"""Valentin Vallageas: Imaging and orthopedics research laboratory; David R Labbe: Ecole de technologie superieure; Rachid Aissaoui: École de technologie supérieure""","""Embodiment refers to the sensation of owning, controlling, and perceiving a virtual or artificial body as one's own. This study investigates how embodying an avatar with a leg twice its normal length affects proprioception, with congruent or incongruent visuotactile stimuli. Preliminary results (n = 10) show that participants experienced embodiment with a lengthened virtual leg, regardless of congruence of stimuli. A proprioceptive drift of 31.2 cm toward the virtual foot was observed. These findings extend research on upper-body proprioception to include virtual lower-limb deformations.""","""Proprioception Drift in Virtual Reality: An Experiment with an Unrealistically Long Leg""","""Perception and Cognition""",ODDIbtStt6w
1255,"""Gaku Fukui: The University of Tokyo; Maki Ogawa: the University of Tokyo; Keigo Matsumoto: The University of Tokyo; Takuto Nakamura: The University of Tokyo; Takuji Narumi: the University of Tokyo; Hideaki Kuzuoka: The University of Tokyo""","""Hand Redirection (HR) techniques subtly alter a user’s real hand movement to deviate from its virtual representation, enhancing passive haptics in virtual environments. However, noticeable discrepancies can reduce immersion. Inspired by redirected walking research demonstrating that knee supports can influence the perception of manipulation, this study explores whether shoulder and elbow supports can expand the detection threshold in horizontal HR. A user study revealed that shoulder supports significantly increase the DT, offering a practical, stimulus-free method to enhance HR. These findings highlight the potential of leveraging joint constraints to improve immersion without relying on external stimuli.""","""The Effect of Elbow and Shoulder Supports on Detection Threshold of Hand Redirection""","""Perception and Cognition""",fLYlG5EngQQ
1256,"""Hyunjo Bang: Konkuk University; Mingyu Kwon: Konkuk University; Seunghan Lee: Konkuk University; HyungSeok Kim: Konkuk University""","""This paper investigates the phenomenon of perceptual distortion of time caused by the stimulation of the senses in a virtual environment. We examine how time perception changes with sensory stimuli at varying intervals, focusing on visual, auditory, tactile, and visual-auditory stimuli, as well as the impact of complex visual event frequencies. Participants' time perception before and during the experiment was compared. The analysis showed that visual stimuli induced time compression, and auditory stimuli induced time dilation. Time dilation was more effectively induced than time compression. Based on these results, personalizing the time interval of sensory stimuli may maximize the time distortion effect.""","""Perception of Time in Virtual Reality with Different Sensory Stimulation""","""Perception and Cognition""",-J6qG6nYojI
1257,"""Ifigeneia Mavridou: Tilburg University; Ellen Seiss: Bournemouth University; Dr. Giuseppe Ugazio: University of Geneva; Mark Harpster: Bongiovi Acoustics Labs; Phillip Brown: Tilburg University; Sophia Cox: Emteq Labs; Christine Erie: Bongiovi Accoustics Labs; David Lopez Jr: Bongiovi Accoustics Labs; Ryan Copt: Bongiovi Accoustics Labs; Charles Nduka: Emteq Ltd; James Hughes: Bongiovi Accoustics Labs; Joseph Butera III: Bongiovi Accoustics Labs; Daniel N Weiss: Bongiovi Accoustics Labs""","""This study examines the effects of software-based localised audio enhancement in VR on spatial audio, immersion, and affective responses. Sixty-eight participants took part in a VR game (Job Simulator) and reported improved sound quality, immersion, and localization under enhanced audio conditions. Physiological measures revealed significant effects on emotional valence, highlighting enhanced audio as a cost-effective way to improve auditory involvement and emotional engagement in VR without additional hardware""","""“Did you hear that?”: Investigating localized spatial audio effects on enhancing immersive and affective user experiences in virtual reality""","""Perception and Cognition""",-Gids0GMIhs